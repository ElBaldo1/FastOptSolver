{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SECTION 0 — Setup & Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression Solver Benchmark\n",
    "\n",
    "This notebook demonstrates the implementation and performance comparison of various optimization algorithms for solving regularized linear regression problems.\n",
    "\n",
    "We test:\n",
    "- ISTA (Iterative Soft-Thresholding Algorithm)\n",
    "- FISTA (Fast ISTA)\n",
    "- Gradient Descent (as a baseline)\n",
    "- (Optionally) L-BFGS for smooth problems\n",
    "\n",
    "The models handled include:\n",
    "- LASSO (ℓ₁-regularized least squares)\n",
    "- Ridge (ℓ₂-regularized least squares)\n",
    "- Elastic Net (ℓ₁ + ℓ₂ regularization)\n",
    "\n",
    "---\n",
    "\n",
    "This notebook relies on external modular implementations found in the `prox/` and `solvers/` directories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Imports ---\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# --- Prox operators ---\n",
    "from prox.prox_l1 import prox_l1\n",
    "from prox.prox_l2 import prox_l2\n",
    "from prox.prox_elasticnet import prox_elasticnet\n",
    "\n",
    "# --- Solvers ---\n",
    "from solvers.ista import ista\n",
    "from solvers.fista import fista\n",
    "from solvers.gradient import gradient_descent\n",
    "from solvers.lbfgs import lbfgs_solver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SECTION 1 — Dataset Loading & Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Dataset Loading & Preprocessing\n",
    "\n",
    "We use a real-world dataset for benchmarking the performance of our solvers.\n",
    "The dataset is expected to be a CSV file located in the `data/` folder.\n",
    "\n",
    "We perform:\n",
    "- Feature-target separation\n",
    "- Standardization of input features\n",
    "- Train/test split (80/20)\n",
    "\n",
    "**Note**: Modify the path and column names below according to your specific dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# --- Load dataset (customize this part!) ---\n",
    "dataset_path = Path(\"data\") / \"your_dataset.csv\"  # TODO: replace with actual filename\n",
    "df = pd.read_csv(dataset_path)\n",
    "\n",
    "# --- Configure target and features ---\n",
    "target_column = \"target\"  # TODO: replace with actual target name\n",
    "feature_columns = [col for col in df.columns if col != target_column]\n",
    "\n",
    "X = df[feature_columns].values\n",
    "y = df[target_column].values\n",
    "\n",
    "# --- Standardize features ---\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# --- Train/test split ---\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# --- Confirm shapes ---\n",
    "print(f\"Train shape: {X_train.shape}, Test shape: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SECTION 2 — Solver Runner Utility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Solver Runner\n",
    "\n",
    "This section defines the `run_solver` function, which:\n",
    "- Selects the appropriate solver (ISTA, FISTA, etc.)\n",
    "- Selects the correct proximity operator based on the regularization model\n",
    "- Runs the solver and collects performance metrics\n",
    "\n",
    "All solvers return:\n",
    "- Final solution `x`\n",
    "- Objective function history\n",
    "- Execution time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_solver(\n",
    "    method: str,\n",
    "    model: str,\n",
    "    A: np.ndarray,\n",
    "    b: np.ndarray,\n",
    "    lam: float,\n",
    "    alpha: float = 0.5,\n",
    "    max_iter: int = 1000,\n",
    "    tol: float = 1e-6,\n",
    "    verbose: bool = False\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    method: 'ista', 'fista', 'gradient', 'lbfgs'\n",
    "    model: 'lasso', 'ridge', 'elasticnet'\n",
    "    lam: regularization parameter\n",
    "    alpha: elastic net mixing parameter (0 = ridge, 1 = lasso)\n",
    "    \"\"\"\n",
    "    # Select proximity operator\n",
    "    if model == \"lasso\":\n",
    "        prox_op = prox_l1\n",
    "    elif model == \"ridge\":\n",
    "        prox_op = prox_l2\n",
    "    elif model == \"elasticnet\":\n",
    "        prox_op = lambda x, lam_: prox_elasticnet(x, lam_, alpha)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model '{model}'\")\n",
    "\n",
    "    # Select solver\n",
    "    start_time = time.time()\n",
    "\n",
    "    if method == \"ista\":\n",
    "        result = ista(A, b, lam, prox_op, max_iter, tol, verbose)\n",
    "    elif method == \"fista\":\n",
    "        result = fista(A, b, lam, prox_op, max_iter, tol, verbose)\n",
    "    elif method == \"gradient\":\n",
    "        result = gradient_descent(A, b, lam, max_iter, tol, verbose)\n",
    "    elif method == \"lbfgs\":\n",
    "        result = lbfgs_solver(A, b, lam, model=model)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown method '{method}'\")\n",
    "\n",
    "    result[\"time\"] = time.time() - start_time\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SECTION 3 — Run Solvers on the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Run Solvers\n",
    "\n",
    "We now run the selected optimization algorithms on the chosen dataset.\n",
    "\n",
    "You can choose:\n",
    "- The regularization model (`lasso`, `ridge`, `elasticnet`)\n",
    "- The optimization method (`ista`, `fista`, `gradient`, `lbfgs`)\n",
    "- The regularization parameter `lambda`\n",
    "- (Optional) Elastic Net mixing parameter `alpha`\n",
    "\n",
    "The results include the final solution, convergence history, runtime, and more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration ---\n",
    "model = \"lasso\"           # 'lasso', 'ridge', 'elasticnet'\n",
    "method = \"fista\"          # 'ista', 'fista', 'gradient', 'lbfgs'\n",
    "lam = 0.1                 # regularization strength\n",
    "alpha = 0.5               # only used for elastic net\n",
    "\n",
    "# --- Run selected solver ---\n",
    "result = run_solver(\n",
    "    method=method,\n",
    "    model=model,\n",
    "    A=X_train,\n",
    "    b=y_train,\n",
    "    lam=lam,\n",
    "    alpha=alpha,\n",
    "    max_iter=1000,\n",
    "    tol=1e-6,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# --- Unpack results ---\n",
    "x_sol = result[\"x\"]\n",
    "history = result.get(\"history\", [])\n",
    "elapsed_time = result[\"time\"]\n",
    "\n",
    "print(f\"Final objective value: {history[-1] if history else 'N/A'}\")\n",
    "print(f\"Elapsed time: {elapsed_time:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SECTION 4 — Evaluation Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluation\n",
    "\n",
    "Once the solver has produced a solution vector `x`, we evaluate its quality on the test set.\n",
    "\n",
    "We report:\n",
    "- Test Mean Squared Error (MSE)\n",
    "- Training MSE (optional)\n",
    "- Sparsity of the solution: percentage of near-zero elements in `x`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_sparsity(x: np.ndarray, threshold: float = 1e-6) -> float:\n",
    "    \"\"\"Return the fraction of coefficients in x that are effectively zero.\"\"\"\n",
    "    return np.sum(np.abs(x) < threshold) / len(x)\n",
    "\n",
    "# --- Predict on test set ---\n",
    "y_pred = X_test @ x_sol\n",
    "test_mse = mean_squared_error(y_test, y_pred)\n",
    "train_mse = mean_squared_error(y_train, X_train @ x_sol)\n",
    "sparsity = compute_sparsity(x_sol)\n",
    "\n",
    "# --- Print metrics ---\n",
    "print(f\"Train MSE: {train_mse:.4f}\")\n",
    "print(f\"Test MSE:  {test_mse:.4f}\")\n",
    "print(f\"Sparsity:  {sparsity * 100:.2f}% zeros in solution vector\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SECTION 5 — Convergence and Sparsity Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Convergence and Sparsity Plots\n",
    "\n",
    "We visualize:\n",
    "- The convergence of the objective value over iterations\n",
    "- The sparsity of the solution (optional: per iteration if tracked)\n",
    "\n",
    "This helps assess:\n",
    "- The speed of convergence\n",
    "- The stability and smoothness of the solver trajectory\n",
    "- How aggressively each method induces sparsity (for LASSO/Elastic Net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Plot objective function history ---\n",
    "if history:\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.plot(history, label=f\"{method.upper()} - {model}\")\n",
    "    plt.xlabel(\"Iteration\")\n",
    "    plt.ylabel(\"Objective Value\")\n",
    "    plt.title(\"Convergence Curve\")\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No convergence history available to plot.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional Extension for Later\n",
    "If you later track sparsity at each iteration inside your solver (e.g., by appending to result[\"sparsity_history\"]), you could add:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparsity_history = result.get(\"sparsity_history\", None)\n",
    "if sparsity_history:\n",
    "    plt.plot(sparsity_history)\n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SECTION 6 — Summary & Multi-Solver Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Summary and Comparison\n",
    "\n",
    "To compare multiple solvers or models, we run several configurations and collect:\n",
    "\n",
    "- Final objective value\n",
    "- Runtime (in seconds)\n",
    "- Test MSE\n",
    "- Solution sparsity\n",
    "\n",
    "We display these results in a summary table for easy analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Benchmark Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "methods = [\"ista\", \"fista\", \"gradient\", \"lbfgs\"]\n",
    "model = \"lasso\"  # or 'ridge', 'elasticnet'\n",
    "lam = 0.1\n",
    "alpha = 0.5\n",
    "\n",
    "benchmark_results = []\n",
    "\n",
    "for method in methods:\n",
    "    print(f\"\\nRunning: {method.upper()}...\")\n",
    "    result = run_solver(\n",
    "        method=method,\n",
    "        model=model,\n",
    "        A=X_train,\n",
    "        b=y_train,\n",
    "        lam=lam,\n",
    "        alpha=alpha,\n",
    "        max_iter=1000,\n",
    "        tol=1e-6,\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    x_sol = result[\"x\"]\n",
    "    y_pred = X_test @ x_sol\n",
    "    test_mse = mean_squared_error(y_test, y_pred)\n",
    "    sparsity = compute_sparsity(x_sol)\n",
    "    time_taken = result[\"time\"]\n",
    "    final_obj = result.get(\"history\", [None])[-1]\n",
    "\n",
    "    benchmark_results.append({\n",
    "        \"method\": method,\n",
    "        \"final_obj\": final_obj,\n",
    "        \"test_mse\": test_mse,\n",
    "        \"sparsity\": sparsity,\n",
    "        \"time (s)\": time_taken\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display Results Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Show results as a DataFrame ---\n",
    "import pandas as pd\n",
    "\n",
    "df_results = pd.DataFrame(benchmark_results)\n",
    "df_results = df_results.sort_values(by=\"test_mse\")\n",
    "display(df_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
